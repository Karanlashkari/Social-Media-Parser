{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dc703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import streamlit as st\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import emoji\n",
    "from ntscraper import Nitter\n",
    "import pandas as pd\n",
    "import googleapiclient.discovery\n",
    "from langdetect import detect\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe0fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Streamlit background\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    .stApp {\n",
    "        background-image: url('https://images.rawpixel.com/image_800/czNmcy1wcml2YXRlL3Jhd3BpeGVsX2ltYWdlcy93ZWJzaXRlX2NvbnRlbnQvbHIvdjU0NmJhdGNoMy1teW50LTM0LWJhZGdld2F0ZXJjb2xvcl8xLmpwZw.jpg');\n",
    "        background-size: cover;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd57c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the scraper\n",
    "scraper = Nitter()\n",
    "\n",
    "# Define chat abbreviations and text processing\n",
    "chat_words = {\n",
    "    \"lol\": \"laugh out loud\", \"brb\": \"be right back\", \"ttyl\": \"talk to you later\",\n",
    "    \"gtg\": \"got to go\", \"btw\": \"by the way\", \"omg\": \"oh my god\", \"idk\": \"i don't know\",\n",
    "}\n",
    "\n",
    "def chat_convo(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        w = w.strip(string.punctuation)\n",
    "        if w.lower() in chat_words:\n",
    "            new_text.append(chat_words[w.lower()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def demojize_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    text = chat_convo(text)\n",
    "    text = demojize_text(text)\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model and tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            r\"C:\\Users\\acer\\Desktop\\MegaProj\\bertweet_sentiment_model\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            r\"C:\\Users\\acer\\Desktop\\MegaProj\\bertweet_sentiment_model\"\n",
    "        )\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model/tokenizer: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46879dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions using the model\n",
    "def predict(text, model, tokenizer):\n",
    "    if model is None or tokenizer is None:\n",
    "        return -1\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Function to extract video ID from YouTube URL\n",
    "def extract_video_id(url):\n",
    "    url = url.split('?')[0]  # Remove any query parameters\n",
    "    patterns = [\n",
    "        r'(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/]+\\/.+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([^\"&?\\/\\s]{11})',\n",
    "        r'^([^\"&?\\/\\s]{11})$'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None  # Return None if no valid ID is found\n",
    "\n",
    "# Function to scrape YouTube comments\n",
    "def scrape_youtube_comments(video_id, language='en'):\n",
    "    try:\n",
    "        DEVELOPER_KEY = \" AIzaSyB6-LJLUT2tNwwbqGrt8VaYIrpT81Iaod0\"  # Replace with your API key\n",
    "        youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=DEVELOPER_KEY)\n",
    "        comments = []\n",
    "        nextPageToken = None\n",
    "\n",
    "        while len(comments) < 100:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=nextPageToken\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "                try:\n",
    "                    if detect(comment) == language:\n",
    "                        comments.append(comment)\n",
    "                        if len(comments) >= 100:\n",
    "                            break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            nextPageToken = response.get('nextPageToken')\n",
    "            if not nextPageToken:\n",
    "                break\n",
    "\n",
    "        return comments\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error scraping YouTube comments: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Install spaCy model if necessary\n",
    "def install_spacy_model():\n",
    "    try:\n",
    "        import spacy\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        st.warning(\"Installing spaCy English model...\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "            st.success(\"Model installed successfully!\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            st.error(\"Failed to install. Please run manually:\")\n",
    "            st.code(\"python -m spacy download en_core_web_sm\")\n",
    "            st.stop()\n",
    "\n",
    "# Main function to handle Streamlit interface\n",
    "def main():\n",
    "    # Ensure necessary models are loaded\n",
    "    install_spacy_model()\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load spaCy model: {str(e)}\")\n",
    "        st.stop()\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "    st.sidebar.title(\"Options\")\n",
    "    option = st.sidebar.radio(\"Choose an option\", \n",
    "                            [\"Home\", \"Tweets Analysis\", \"YouTube Comments Analysis\", \"File Upload\", \"About\"])\n",
    "\n",
    "    if option == \"Home\":\n",
    "        st.title(\"Detection of Hate Speech on Social Media\")\n",
    "        user_input = st.text_area(\"Enter text here to check if it is Hate Speech\")\n",
    "        if st.button(\"Check for Hate Text\"):\n",
    "            if user_input.strip() == \"\":\n",
    "                st.warning(\"Please enter some text to analyze.\")\n",
    "            else:\n",
    "                prediction = predict(user_input)\n",
    "                if prediction == -1:\n",
    "                    st.error(\"Model not loaded properly.\")\n",
    "                elif prediction == 0:\n",
    "                    st.success(\"**Unlikely to be Hate Speech**\")\n",
    "                else:\n",
    "                    st.error(\"**Potential Hate Speech**\")\n",
    "\n",
    "    elif option == \"Tweets Analysis\":\n",
    "        st.title(\"Tweets Prediction for Textual Hate Speech\")\n",
    "        st.write(\"Enter term or a username to scrape tweets and predict for any potential hate speech\")\n",
    "       # Fixing duplicate widget ID error by adding a unique key\n",
    "        scrape_option = st.radio(\"Choose an option\", [\"Term\", \"Username\"], key=\"scrape_option\")\n",
    "        input_text = st.text_input(\"Enter term\" if scrape_option == \"Term\" else \"Enter Twitter username\")\n",
    "        if st.button(\"Scrape Tweets\"):\n",
    "            if input_text.strip() == \"\":\n",
    "                st.warning(\"Please enter a valid input.\")\n",
    "            else:\n",
    "                tweets = scraper.get_tweets(input_text, mode='term' if scrape_option == \"Term\" else 'user', number=10)\n",
    "                if tweets is not None:\n",
    "                    tweet_texts = [tweet['text'] for tweet in tweets['tweets']]\n",
    "                    predictions = [predict(text) for text in tweet_texts]\n",
    "                    df = pd.DataFrame({'Tweet': tweet_texts, 'Prediction': predictions})\n",
    "                    st.write(\"Predictions for Tweets:\")\n",
    "                    st.write(df)\n",
    "                else:\n",
    "                    st.error(\"Failed to retrieve tweets. Please check the input and try again.\")\n",
    "\n",
    "    elif option == \"YouTube Comments Analysis\":\n",
    "        st.title(\"YouTube Comments Analysis\")\n",
    "        st.write(\"Enter a YouTube video link to scrape comments and make predictions\")\n",
    "        vid_input = st.text_area(\"Enter video link here\")\n",
    "        if st.button(\"Scrape comments\"):\n",
    "            if vid_input.strip() == \"\":\n",
    "                st.warning(\"Please enter a valid link\")\n",
    "            else:\n",
    "                video_id = extract_video_id(vid_input)\n",
    "                if video_id:\n",
    "                    comments = scrape_youtube_comments(video_id)\n",
    "                    if comments:\n",
    "                        predictions = [predict(comment, model, tokenizer) for comment in comments]\n",
    "                        df = pd.DataFrame({'Comment': comments, 'Prediction': predictions})\n",
    "                        st.write(df)\n",
    "                    else:\n",
    "                        st.error(\"No comments found or error fetching comments.\")\n",
    "                else:\n",
    "                    st.error(\"Invalid video ID. Please check the YouTube link.\")\n",
    "\n",
    "    elif option == \"File Upload\":\n",
    "        st.title(\"Upload files for Prediction\")\n",
    "        st.write(\"Predict hate text from data present in a .csv file\")\n",
    "        uploaded_file = st.file_uploader(\"Choose a file\", type=[\"csv\"])\n",
    "        if uploaded_file is not None:\n",
    "            df = pd.read_csv(uploaded_file, usecols=['text'])\n",
    "            st.write(\"Preview of the 'text' column from the uploaded file:\")\n",
    "            st.write(df.head())\n",
    "            predictions = [predict(text) for text in df['text']]\n",
    "            df['Prediction'] = predictions\n",
    "            st.write(\"Predictions for uploaded data:\")\n",
    "            st.write(df)\n",
    "\n",
    "    elif option == \"About\":\n",
    "        st.title(\"Detection of Hate Speech Against LGBT+ on Social Media\")\n",
    "        st.write(\"\"\"\n",
    "        [Project description here remains unchanged.]\n",
    "        \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def install_spacy_model():\n",
    "    try:\n",
    "        import spacy\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        st.warning(\"Installing spaCy English model...\")\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
    "            st.success(\"Model installed successfully!\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            st.error(\"Failed to install. Please run manually:\")\n",
    "            st.code(\"python -m spacy download en_core_web_sm\")\n",
    "            st.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    st.sidebar.title(\"Options\")\n",
    "    option = st.sidebar.radio(\"Choose an option\", \n",
    "                            [\"Home\", \"Tweets Analysis\", \"YouTube Comments Analysis\", \"File Upload\", \"About\"])\n",
    "\n",
    "    if option == \"Home\":\n",
    "        st.title(\"Detection of Hate Speech on Social Media\")\n",
    "        user_input = st.text_area(\"Enter text here to check if it is Hate Speech\")\n",
    "        if st.button(\"Check for Hate Text\"):\n",
    "            if user_input.strip() == \"\":\n",
    "                st.warning(\"Please enter some text to analyze.\")\n",
    "            else:\n",
    "                prediction = predict(user_input)\n",
    "                if prediction == -1:\n",
    "                    st.error(\"Model not loaded properly.\")\n",
    "                elif prediction == 0:\n",
    "                    st.success(\"**Unlikely to be Hate Speech**\")\n",
    "                else:\n",
    "                    st.error(\"**Potential Hate Speech**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbea110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# install_spacy_model()\n",
    "\n",
    "# try:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "# except Exception as e:\n",
    "#     st.error(f\"Failed to load spaCy model: {str(e)}\")\n",
    "#     st.stop()\n",
    "\n",
    "# model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6db98c-6a1f-46ad-b1a6-9615dd8b8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"Hate_Speech_App (Running).ipynb\"\n",
    "output_script_path = \"hate_speech_app.py\"\n",
    "\n",
    "with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "code_cells = [cell for cell in notebook.cells if cell.cell_type == \"code\"]\n",
    "with open(output_script_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for cell in code_cells:\n",
    "        f.write(cell.source + \"\\n\\n\")\n",
    "\n",
    "print(\"✅ Streamlit app code saved as 'hate_speech_app.py'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878b5bc-83af-4957-89ab-9bf164095302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # shows the folder where files are saved\n",
    "print(os.listdir())  # lists files in that folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd723cd-275c-4858-9565-2cb8d44ae0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work from inside Jupyter, but will remind you what to run:\n",
    "print(\"Now open a terminal and run this command:\")\n",
    "print(\"streamlit run hate_speech_app.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee72df-d23f-4d3d-8768-af6befc2987e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
